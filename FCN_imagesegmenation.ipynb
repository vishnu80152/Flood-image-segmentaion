{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21241,"status":"ok","timestamp":1703916903388,"user":{"displayName":"Vishnu Madhusudanan","userId":"04314106796719800512"},"user_tz":-330},"id":"3M71HjvQVA7u","outputId":"0aa25b1a-2927-4a29-b86b-cb20519d99c9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":772},"executionInfo":{"elapsed":3539,"status":"error","timestamp":1703918943783,"user":{"displayName":"Vishnu Madhusudanan","userId":"04314106796719800512"},"user_tz":-330},"id":"XIp7KTQyceeX","outputId":"4af81d00-87c4-49a9-8724-540f47efe535"},"outputs":[{"name":"stdout","output_type":"stream","text":["FCN(\n","  (encoder): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU(inplace=True)\n","    (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (3): ReLU(inplace=True)\n","    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (bottleneck): Sequential(\n","    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU(inplace=True)\n","    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (3): ReLU(inplace=True)\n","  )\n","  (decoder): Sequential(\n","    (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU(inplace=True)\n","    (2): Upsample(scale_factor=2.0, mode='bilinear')\n","    (3): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n","  )\n",")\n","GPU is available\n"]},{"ename":"RuntimeError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-cf2e70f3fb1b>\u001b[0m in \u001b[0;36m<cell line: 141>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Move data to GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfcn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;31m# BCEWithLogitsLoss handles sigmoid activation internally, no need to normalize masks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-28-cf2e70f3fb1b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# Forward pass through the encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# Forward pass through the bottleneck\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    454\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 456\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    457\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from PIL import Image\n","import os\n","\n","\n","\n","class FCN(nn.Module):\n","    def __init__(self, num_classes):\n","        super(FCN, self).__init__()\n","\n","        # Encoder (downsampling)\n","        self.encoder = nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            # Add more convolutional layers as needed\n","        )\n","\n","        # Bottleneck (no pooling or downsampling)\n","        self.bottleneck = nn.Sequential(\n","            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","        )\n","\n","        # Decoder (upsampling)\n","        self.decoder = nn.Sequential(\n","            # Add more layers based on the encoder\n","            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n","            nn.Conv2d(64, num_classes, kernel_size=1)\n","        )\n","\n","    def forward(self, x):\n","        # Forward pass through the encoder\n","        x1 = self.encoder(x)\n","\n","        # Forward pass through the bottleneck\n","        x2 = self.bottleneck(x1)\n","\n","        # Forward pass through the decoder\n","        x3 = self.decoder(x2)\n","\n","        return x3\n","\n","# Instantiate the model\n","num_classes = 1  # Assuming binary segmentation (flood or non-flood)\n","fcn_model = FCN(num_classes)\n","\n","# Print the model architecture\n","print(fcn_model)\n","\n","import torch\n","\n","# Check if GPU is available\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","    print(\"GPU is available\")\n","else:\n","    device = torch.device(\"cpu\")\n","    print(\"GPU is not available, using CPU\")\n","\n","fcn_model.to(device)\n","\n","transform=transforms.Compose([\n","    transforms.Resize((224,224)),\n","    transforms.ToTensor(),\n","    # transforms.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225])\n","])\n","\n","class FloodDataset(torch.utils.data.Dataset):\n","  def __init__(self,root_dir,transform=None):\n","    self.root_dir=root_dir\n","    self.image_folder=os.path.join(root_dir,\"Image\")\n","    self.mask_folder=os.path.join(root_dir,\"Mask\")\n","    self.image_paths=sorted([os.path.join(self.image_folder,img) for img in os.listdir(self.image_folder)])\n","    self.mask_paths = sorted([os.path.join(self.mask_folder, mask) for mask in os.listdir(self.mask_folder)])\n","    self.transform=transform\n","\n","  def __len__(self):\n","    return(len(self.image_paths))\n","\n","  def __getitem__(self,idx):\n","    image_path=self.image_paths[idx]\n","    mask_path=self.mask_paths[idx]\n","\n","    image=Image.open(image_path).convert(\"RGB\")\n","    mask=Image.open(mask_path).convert(\"L\")\n","    if self.transform:\n","      image=self.transform(image)\n","      mask=self.transform(mask)\n","\n","    return image,mask\n","class FloodDataset(torch.utils.data.Dataset):\n","  def __init__(self,root_dir,transform=None):\n","    self.root_dir=root_dir\n","    self.image_folder=os.path.join(root_dir,\"Image\")\n","    self.mask_folder=os.path.join(root_dir,\"Mask\")\n","    self.image_paths=sorted([os.path.join(self.image_folder,img) for img in os.listdir(self.image_folder)])\n","    self.mask_paths = sorted([os.path.join(self.mask_folder, mask) for mask in os.listdir(self.mask_folder)])\n","    self.transform=transform\n","\n","  def __len__(self):\n","    return(len(self.image_paths))\n","\n","  def __getitem__(self,idx):\n","    image_path=self.image_paths[idx]\n","    mask_path=self.mask_paths[idx]\n","\n","    image=Image.open(image_path).convert(\"RGB\")\n","    mask=Image.open(mask_path).convert(\"L\")\n","    if self.transform:\n","      image=self.transform(image)\n","      mask=self.transform(mask)\n","\n","    return image,mask\n","\n","flood_dataset = FloodDataset(root_dir=r\"/content/drive/MyDrive/deeplearnig/dataset\", transform=transform)\n","\n","train_loader = DataLoader(flood_dataset, batch_size=4, shuffle=True)\n","\n","# Instantiate the FCN model\n","num_classes = 1  # Assuming binary segmentation (flood or non-flood)\n","fcn_model = FCN(num_classes)\n","\n","# Loss function and optimizer\n","criterion = nn.BCEWithLogitsLoss()\n","optimizer = optim.Adam(fcn_model.parameters(), lr=0.001)\n","\n","# Training loop\n","num_epochs = 10\n","\n","for epoch in range(num_epochs):\n","    fcn_model.train()\n","    for images, masks in train_loader:\n","        images, masks = images.to(device), masks.to(device)  # Move data to GPU\n","        optimizer.zero_grad()\n","        outputs = fcn_model(images)\n","\n","        # BCEWithLogitsLoss handles sigmoid activation internally, no need to normalize masks\n","        loss = criterion(outputs, masks)\n","        loss.backward()\n","        optimizer.step()\n","\n","    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item():.4f}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1288,"status":"ok","timestamp":1703920502358,"user":{"displayName":"Vishnu Madhusudanan","userId":"04314106796719800512"},"user_tz":-330},"id":"mvJjJcWdfeK2","outputId":"2f342c9a-c3c0-4ad0-995e-bc42723b3770"},"outputs":[{"name":"stdout","output_type":"stream","text":["FCN(\n","  (encoder): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU(inplace=True)\n","    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (5): ReLU(inplace=True)\n","    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (9): ReLU(inplace=True)\n","    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (12): ReLU(inplace=True)\n","    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (bottleneck): Sequential(\n","    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU(inplace=True)\n","    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (5): ReLU(inplace=True)\n","  )\n","  (decoder): Sequential(\n","    (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU(inplace=True)\n","    (3): Upsample(scale_factor=2.0, mode='bilinear')\n","    (4): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (6): ReLU(inplace=True)\n","    (7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (9): ReLU(inplace=True)\n","    (10): Upsample(scale_factor=2.0, mode='bilinear')\n","    (11): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n","  )\n",")\n","Using device: cuda\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from PIL import Image\n","import os\n","\n","class FCN(nn.Module):\n","    def __init__(self, num_classes):\n","        super(FCN, self).__init__()\n","\n","        # Encoder (downsampling)\n","        self.encoder = nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(inplace=True),\n","\n","            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(inplace=True),\n","\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(inplace=True),\n","\n","            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(inplace=True),\n","\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","            # Add more convolutional layers as needed\n","        )\n","\n","        # Bottleneck (no pooling or downsampling)\n","        self.bottleneck = nn.Sequential(\n","            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(inplace=True),\n","\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(inplace=True),\n","        )\n","\n","        # Decoder (upsampling)\n","        self.decoder = nn.Sequential(\n","            # Add more layers based on the encoder\n","            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(inplace=True),\n","\n","            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n","\n","            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(inplace=True),\n","\n","            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(inplace=True),\n","\n","            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n","\n","            nn.Conv2d(64, num_classes, kernel_size=1)\n","        )\n","\n","    def forward(self, x):\n","        # Forward pass through the encoder\n","        x1 = self.encoder(x)\n","\n","        # Forward pass through the bottleneck\n","        x2 = self.bottleneck(x1)\n","\n","        # Forward pass through the decoder\n","        x3 = self.decoder(x2)\n","\n","        return x3\n","\n","# Instantiate the model\n","num_classes = 1  # Assuming binary segmentation (flood or non-flood)\n","fcn_model = FCN(num_classes)\n","\n","# Print the model architecture\n","print(fcn_model)\n","\n","# Check if GPU is available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# Move the model to the specified device\n","fcn_model.to(device)\n","\n","# Define transformations for images and masks\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7J8Rq5MIgQdo"},"outputs":[],"source":["\n","class FloodDataset(Dataset):\n","    def __init__(self, root_dir, transform=None):\n","        self.root_dir = root_dir\n","        self.image_folder = os.path.join(root_dir, \"Image\")\n","        self.mask_folder = os.path.join(root_dir, \"Mask\")\n","        self.image_paths = sorted([os.path.join(self.image_folder, img) for img in os.listdir(self.image_folder)])\n","        self.mask_paths = sorted([os.path.join(self.mask_folder, mask) for mask in os.listdir(self.mask_folder)])\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        image_path = self.image_paths[idx]\n","        mask_path = self.mask_paths[idx]\n","\n","        image = Image.open(image_path).convert(\"RGB\")\n","        mask = Image.open(mask_path).convert(\"L\")\n","\n","        if self.transform:\n","            image = self.transform(image)\n","            mask = self.transform(mask)\n","\n","        return image, mask\n","\n","# Create the dataset and dataloader\n","flood_dataset = FloodDataset(root_dir=\"/content/drive/MyDrive/deeplearnig/dataset\", transform=transform)\n","train_loader = DataLoader(flood_dataset, batch_size=4, shuffle=True)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G654p63Oi8Hs"},"outputs":[],"source":["train_size = int(0.8 * len(flood_dataset))\n","test_size = len(flood_dataset) - train_size\n","train_dataset, test_dataset = torch.utils.data.random_split(flood_dataset, [train_size, test_size])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vrIDPFjgjGlp"},"outputs":[],"source":["train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":102134,"status":"ok","timestamp":1703920615759,"user":{"displayName":"Vishnu Madhusudanan","userId":"04314106796719800512"},"user_tz":-330},"id":"ZjUVxt1wgS9k","outputId":"68b80013-b458-4467-bf40-dedc73879f76"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10, Loss: 0.3581\n","Epoch 2/10, Loss: 0.3651\n","Epoch 3/10, Loss: 0.3919\n","Epoch 4/10, Loss: 0.4163\n","Epoch 5/10, Loss: 0.3016\n","Epoch 6/10, Loss: 0.3317\n","Epoch 7/10, Loss: 0.6133\n","Epoch 8/10, Loss: 0.2332\n","Epoch 9/10, Loss: 0.5030\n","Epoch 10/10, Loss: 0.3008\n"]}],"source":["# Loss function and optimizer\n","criterion = nn.BCEWithLogitsLoss()\n","optimizer = optim.Adam(fcn_model.parameters(), lr=0.001)\n","\n","# Training loop\n","num_epochs = 10\n","\n","for epoch in range(num_epochs):\n","    fcn_model.train()\n","    for images, masks in train_loader:\n","        images, masks = images.to(device), masks.to(device)  # Move data to GPU\n","        optimizer.zero_grad()\n","        outputs = fcn_model(images)\n","\n","        # BCEWithLogitsLoss handles sigmoid activation internally, no need to normalize masks\n","        loss = criterion(outputs, masks)\n","        loss.backward()\n","        optimizer.step()\n","\n","    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item():.4f}')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":135953,"status":"ok","timestamp":1703920948773,"user":{"displayName":"Vishnu Madhusudanan","userId":"04314106796719800512"},"user_tz":-330},"id":"H-H18F7tl9L4","outputId":"aec7ea2c-ff9a-470a-f0ab-50dcd369ff1d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10, Loss: 0.3667, Accuracy: 0.7867\n","Epoch 2/10, Loss: 0.2842, Accuracy: 0.8462\n","Epoch 3/10, Loss: 0.3269, Accuracy: 0.7880\n","Epoch 4/10, Loss: 0.4751, Accuracy: 0.7496\n","Epoch 5/10, Loss: 0.3508, Accuracy: 0.8100\n","Epoch 6/10, Loss: 0.3477, Accuracy: 0.8098\n","Epoch 7/10, Loss: 0.3087, Accuracy: 0.8474\n","Epoch 8/10, Loss: 0.5528, Accuracy: 0.7589\n","Epoch 9/10, Loss: 0.3434, Accuracy: 0.8042\n","Epoch 10/10, Loss: 0.2231, Accuracy: 0.8870\n"]}],"source":["import torch\n","\n","def calculate_accuracy(predictions, targets, threshold=0.5):\n","    \"\"\"\n","    Calculate accuracy based on predictions and ground truth masks.\n","\n","    Args:\n","    - predictions (torch.Tensor): Model predictions (logits)\n","    - targets (torch.Tensor): Ground truth masks\n","    - threshold (float): Threshold for converting logits to binary predictions\n","\n","    Returns:\n","    - accuracy (float): Accuracy value\n","    \"\"\"\n","    with torch.no_grad():\n","        binary_predictions = (torch.sigmoid(predictions) > threshold).float()\n","        correct_predictions = (binary_predictions == targets).float()\n","\n","        accuracy = correct_predictions.sum() / (targets.numel() + 1e-10)  # Add a small value to avoid division by zero\n","\n","    return accuracy.item()\n","\n","# Example of how to use the accuracy function in your training loop\n","for epoch in range(num_epochs):\n","    fcn_model.train()\n","    for images, masks in train_loader:\n","        images, masks = images.to(device), masks.to(device)  # Move data to GPU\n","        optimizer.zero_grad()\n","        outputs = fcn_model(images)\n","\n","        # BCEWithLogitsLoss handles sigmoid activation internally, no need to normalize masks\n","        loss = criterion(outputs, masks)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Calculate accuracy\n","        accuracy = calculate_accuracy(outputs, masks)\n","    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item():.4f}, Accuracy: {accuracy:.4f}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uQpO01R3ijle"},"outputs":[],"source":["import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1pk0VLO9WGwXd3eix_Av1UrhpoKa58qpC"},"executionInfo":{"elapsed":72676,"status":"ok","timestamp":1703921049808,"user":{"displayName":"Vishnu Madhusudanan","userId":"04314106796719800512"},"user_tz":-330},"id":"OS5tjMIFidHd","outputId":"d8c69de2-8d4f-4fad-81e3-503d5665f707"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["# Visualize some results\n","fcn_model.eval()\n","with torch.no_grad():\n","    for inputs, targets in test_loader:\n","        inputs, targets = inputs.to(device), targets.to(device)\n","\n","        outputs = fcn_model(inputs)\n","        predictions = (outputs > 0.5).float()  # Apply threshold (adjust as needed)\n","\n","        # Visualize the input image, ground truth mask, and predicted mask\n","        plt.figure(figsize=(12, 4))\n","\n","        plt.subplot(1, 3, 1)\n","        plt.title(\"Input Image\")\n","        plt.imshow(inputs[0].cpu().permute(1, 2, 0))\n","        plt.axis(\"off\")\n","\n","        plt.subplot(1, 3, 2)\n","        plt.title(\"Ground Truth Mask\")\n","        plt.imshow(targets[0].cpu().squeeze(), cmap=\"gray\")\n","        plt.axis(\"off\")\n","\n","        plt.subplot(1, 3, 3)\n","        plt.title(\"Predicted Mask\")\n","        plt.imshow(predictions[0].cpu().squeeze(), cmap=\"gray\")\n","        plt.axis(\"off\")\n","\n","        plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WXLXpyCZiKAT"},"outputs":[],"source":["# Save the trained model\n","torch.save(fcn_model.state_dict(), 'fcn_model.pth')\n"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"authorship_tag":"ABX9TyNkggr185Ij6r2q4J8RrCjk"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}